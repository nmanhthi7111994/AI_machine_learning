{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_Byl2KCgMNl"
      },
      "source": [
        "# Handwritten digits classification using TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCKj0Bn4gMNt"
      },
      "source": [
        "Putting all the concepts we have learned so far, we will see how can use tensorflow to\n",
        "build a neural network to recognize handwritten digits. If you are playing around deep\n",
        "learning off late then you must have come across MNIST dataset. It is being called the hello\n",
        "world of deep learning.\n",
        "\n",
        "It consists of 55,000 data points of handwritten digits (0 to 9).\n",
        "In this section, we will see how can we use our neural network to recognize the\n",
        "handwritten digits and also we will get hang of tensorflow and tensorboard.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIjW-6rRgMNu"
      },
      "source": [
        "## Import required libraries\n",
        "\n",
        "As a first step, let us import all the required libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "upbsikQ1gMNw"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the data\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Display the first image in the training data\n",
        "plt.imshow(x_train[7], cmap='gray')\n",
        "plt.title(f\"Label: {y_train[0]}\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TZ7CXfqXgMNy",
        "outputId": "322d4591-f682-457e-8c41-0cdac4cfb2ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.16.1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Print TensorFlow version\n",
        "print(tf.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFJW-Y-4gMNz"
      },
      "source": [
        "## Load the Dataset\n",
        "\n",
        "In the below code, \"data/mnist\" implies the location where we store the MNIST dataset.\n",
        "one_hot=True implies we are one-hot encoding the labels (0 to 9):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "1vr8pQDqgMN0",
        "outputId": "bb6f4df1-4583-49db-a966-61ed399ae249"
      },
      "outputs": [],
      "source": [
        "\n",
        "mnist = tf.keras.datasets.mnist\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyTPNhNugMN1"
      },
      "source": [
        "Let's check what we got in our data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "oQ7d0fDVgMN2",
        "outputId": "403e02b0-1b4c-4993-d8ce-ba5f39b99795"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of images in training set: 60000\n",
            "Number of labels in training set: 60000\n",
            "Number of images in test set: 10000\n",
            "Number of labels in test set: 10000\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the data\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding if needed\n",
        "y_train_one_hot = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test_one_hot = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "\n",
        "# Print the number of images and labels in the training and test sets\n",
        "print(\"Number of images in training set: {}\".format(x_train.shape[0]))\n",
        "print(\"Number of labels in training set: {}\".format(y_train.shape[0]))\n",
        "\n",
        "print(\"Number of images in test set: {}\".format(x_test.shape[0]))\n",
        "print(\"Number of labels in test set: {}\".format(y_test.shape[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzYV0C64gMN2"
      },
      "source": [
        "We have 55,000 images in the training set and each image is of size 784 and we have 10 labels which are actually 0 to 9. Similarly, we have 10000 images in the test set.\n",
        "\n",
        "Now we plot one image to see how it looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "N8V87y2zgMN3",
        "outputId": "89794a16-6f2c-4a14-c9e2-aa221e4dfe7a"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "img1 = x_train[123].reshape(28, 28)\n",
        "#reshape(28, 28) reshapes the 1-dimensional array representing the image into a 2-dimensional (28x28) array. \n",
        "# This is necessary because the images in MNIST dataset are flattened into 1D arrays (784 pixels).\n",
        "plt.imshow(img1, cmap='Greys')\n",
        "#cmap='Greys' specifies the colormap to use for displaying the image. 'Greys' colormap displays the image in grayscale.\n",
        "plt.title(f\"Label: {y_train[123]}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UprUThESgMN3"
      },
      "source": [
        "## Define the number of neurons in each layer\n",
        "\n",
        "We build a 4 layer neural network with 3 hidden layers and 1 output layer. As the size of\n",
        "the input image is 784. We set the num_input to 784 and since we have 10 handwritten\n",
        "digits (0 to 9), We set 10 neurons in the output layer. We define the number of neurons in\n",
        "each layer as follows,\n",
        "Hidden Layers: Perform computations to transform the input data into a format that makes it easier for the network to learn patterns and relationships in the data.\n",
        "Output Layer: Produces the final predictions or classifications for the input data.\n",
        "## Training Context:\n",
        "During training, the neural network adjusts the weights and biases of each neuron through an optimization process (e.g., using gradient descent and backpropagation). The goal is to minimize a defined loss function (like cross-entropy in classification tasks) and improve the accuracy of predictions on the training data.\n",
        "The specified architecture (number of layers and neurons) influences the capacity of the model to learn complex patterns in the data. Deeper networks with more neurons can potentially capture more intricate relationships but require more computational resources and may be prone to overfitting if not properly regularized.\n",
        "## Summary:\n",
        "In summary, specifying the number of neurons in each layer defines the architecture of a neural network model. Each layer's configuration influences how the model processes and learns from the input data, leading to the model's ability to generalize and make accurate predictions on unseen data during training and evaluation phases. Adjusting these parameters involves balancing model complexity, computational efficiency, and performance metrics such as accuracy and loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "yOVXRgD1gMN4"
      },
      "outputs": [],
      "source": [
        "#number of neurons in input layer\n",
        "num_input = 784\n",
        "\n",
        "#number of neurons in hidden layer 1\n",
        "num_hidden1 = 256\n",
        "\n",
        "#number of neurons in hidden layer 2\n",
        "num_hidden2 = 128\n",
        "\n",
        "#number of neurons in hidden layer 3\n",
        "num_hidden3 = 64\n",
        "\n",
        "#number of neurons in output layer\n",
        "num_output = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfPvk5WIgMN5"
      },
      "source": [
        "## Defining placeholders\n",
        "\n",
        "As we learned, we first need to define the placeholders for input and output. Values for\n",
        "the placeholders will be feed at the run time through feed_dict:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "MLB87He1gMN6"
      },
      "outputs": [],
      "source": [
        "with tf.name_scope('input'):\n",
        "    X = tf.keras.Input(shape=(num_input,), name='X')\n",
        "\n",
        "with tf.name_scope('output'):\n",
        "    Y = tf.keras.Input(shape=(num_output,), name='Y')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvSUyYFugMN6"
      },
      "source": [
        "Since we have a 4 layer network, we have 4 weights and 4 baises. We initialize our weights\n",
        "by drawing values from the truncated normal distribution with a standard deviation of\n",
        "0.1.\n",
        "\n",
        "Remember, the dimensions of the weights matrix should be a number of neurons in the\n",
        "previous layer x number of neurons in the current layer. For instance, the dimension of\n",
        "weight matrix w3 should be the number of neurons in the hidden layer 2 x number of\n",
        "neurons in hidden layer 3.\n",
        "\n",
        "We often define all the weights in a dictionary as given below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "HBLwqaL_gMN6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'w1': <tf.Variable 'weights/weight_1:0' shape=(784, 256) dtype=float32, numpy=\n",
            "array([[-0.13870959, -0.00242143,  0.00517468, ...,  0.01481961,\n",
            "         0.07392079, -0.10576802],\n",
            "       [-0.12054098,  0.02266666, -0.0879983 , ..., -0.03884495,\n",
            "        -0.08364286, -0.01373245],\n",
            "       [-0.03664012,  0.05212152,  0.1895711 , ..., -0.0199808 ,\n",
            "         0.00305892, -0.02154767],\n",
            "       ...,\n",
            "       [-0.03455305,  0.11476151, -0.08022871, ...,  0.00835049,\n",
            "        -0.09517063, -0.11122813],\n",
            "       [-0.11614203,  0.1910951 ,  0.01636261, ..., -0.03500909,\n",
            "        -0.08445861,  0.04111322],\n",
            "       [ 0.03114636, -0.01663359,  0.03409574, ..., -0.11403098,\n",
            "         0.02594852,  0.08080709]], dtype=float32)>, 'w2': <tf.Variable 'weights/weight_2:0' shape=(256, 128) dtype=float32, numpy=\n",
            "array([[ 0.04894988, -0.10916915, -0.10057712, ..., -0.1264234 ,\n",
            "        -0.00883793,  0.08549714],\n",
            "       [ 0.07437675, -0.12402145, -0.00023859, ..., -0.02979183,\n",
            "         0.01107212, -0.1513773 ],\n",
            "       [ 0.01506177, -0.03622283, -0.00472488, ...,  0.02939943,\n",
            "         0.06208126, -0.04538378],\n",
            "       ...,\n",
            "       [-0.0220511 , -0.03294792, -0.05594051, ...,  0.07010975,\n",
            "        -0.06923084, -0.05229529],\n",
            "       [ 0.02023541, -0.18489933, -0.07241574, ...,  0.00992013,\n",
            "        -0.13032144,  0.02462614],\n",
            "       [ 0.08925197,  0.06634622, -0.10337897, ...,  0.03129123,\n",
            "        -0.10093804,  0.01890492]], dtype=float32)>, 'w3': <tf.Variable 'weights/weight_3:0' shape=(128, 64) dtype=float32, numpy=\n",
            "array([[-0.0644198 ,  0.0155031 , -0.07578193, ..., -0.14157279,\n",
            "         0.02901081,  0.00414655],\n",
            "       [-0.06825231,  0.00369413,  0.049684  , ...,  0.02939906,\n",
            "        -0.06410795,  0.06688153],\n",
            "       [-0.01322424, -0.02553715,  0.01499255, ..., -0.06579868,\n",
            "        -0.0380307 ,  0.09529004],\n",
            "       ...,\n",
            "       [ 0.04493809,  0.11663236,  0.04352194, ..., -0.10668545,\n",
            "        -0.115909  , -0.18606642],\n",
            "       [ 0.0647338 , -0.05692101, -0.1060762 , ..., -0.05236218,\n",
            "        -0.00209439,  0.09950912],\n",
            "       [-0.09260863, -0.07420762, -0.01364291, ..., -0.0090555 ,\n",
            "        -0.08399547, -0.10438272]], dtype=float32)>, 'out': <tf.Variable 'weights/weight_4:0' shape=(64, 10) dtype=float32, numpy=\n",
            "array([[ 0.14184664,  0.02722762, -0.11317613,  0.05221224,  0.05509485,\n",
            "         0.00345886, -0.03110907,  0.03488562,  0.06519122,  0.15174615],\n",
            "       [-0.06669999, -0.02655284,  0.01829666, -0.04213307,  0.05143608,\n",
            "         0.03340889,  0.02303356,  0.07062151, -0.04702368,  0.06989033],\n",
            "       [ 0.02881434,  0.07015765,  0.04755431, -0.03816786,  0.02775123,\n",
            "         0.05441089,  0.08381601, -0.09115977,  0.07133694,  0.02255023],\n",
            "       [-0.13402033,  0.07616549,  0.05086758, -0.09260886,  0.09672593,\n",
            "         0.01485634,  0.08825352, -0.06455304, -0.01304711,  0.17274739],\n",
            "       [ 0.16288549,  0.13370587, -0.07751349, -0.11537923, -0.07231464,\n",
            "         0.03702382, -0.11351687,  0.07677595, -0.10623218, -0.07682139],\n",
            "       [-0.08098079,  0.16869508,  0.07690413,  0.020868  , -0.1441385 ,\n",
            "        -0.11743665,  0.00671696,  0.00209906, -0.05261265,  0.06938029],\n",
            "       [ 0.105995  ,  0.09421553,  0.09132238,  0.15155096,  0.00278362,\n",
            "         0.07654853,  0.06863005, -0.020565  , -0.022569  , -0.12122792],\n",
            "       [-0.06579187,  0.1064049 ,  0.02763195,  0.00153192, -0.02886506,\n",
            "        -0.04766838, -0.03052312,  0.11398764,  0.07647264, -0.10496574],\n",
            "       [ 0.11910778,  0.03448984, -0.06292918, -0.05620536,  0.09746177,\n",
            "        -0.03808776, -0.02091108,  0.0954246 ,  0.05644574,  0.11943033],\n",
            "       [-0.1557652 , -0.03429934, -0.10010381, -0.16000299,  0.01645803,\n",
            "        -0.11494674, -0.03122409,  0.02514127,  0.03437882,  0.02134177],\n",
            "       [-0.01045394, -0.09271597, -0.05460068, -0.00807985,  0.04180325,\n",
            "         0.00420264,  0.02784311, -0.0455534 ,  0.03107669,  0.01566656],\n",
            "       [-0.08638696, -0.06256197,  0.05188717, -0.00841946, -0.06960368,\n",
            "        -0.10689249,  0.18761829,  0.08537933, -0.0819293 ,  0.03442128],\n",
            "       [-0.00242747, -0.17988943, -0.09443497,  0.18570428, -0.11998149,\n",
            "         0.02750373,  0.03196803,  0.02669582, -0.05470085,  0.07082114],\n",
            "       [-0.11814421, -0.10955598,  0.02575403, -0.00142748,  0.03213354,\n",
            "        -0.09022506, -0.05934466,  0.03137807, -0.005306  ,  0.15138273],\n",
            "       [-0.02438879, -0.00124862, -0.03859985, -0.08578278,  0.12289138,\n",
            "        -0.11226477, -0.04828303,  0.08819174, -0.08814792,  0.13421176],\n",
            "       [ 0.19606064, -0.14083773, -0.07035249,  0.06514473, -0.07458182,\n",
            "        -0.15244709,  0.1914395 ,  0.07389446,  0.01029891, -0.14825453],\n",
            "       [-0.01267442, -0.01805575, -0.00992128,  0.06183402, -0.06045241,\n",
            "         0.00998584, -0.02805328,  0.09001602,  0.14397666, -0.07994932],\n",
            "       [ 0.01180556, -0.11147907, -0.09030198, -0.04703814, -0.16661438,\n",
            "         0.03078696, -0.12388548, -0.06898162, -0.06633218, -0.09394614],\n",
            "       [ 0.00332103, -0.01388741, -0.14748362,  0.01395536, -0.00808143,\n",
            "         0.13365623, -0.06077366, -0.05504789,  0.04430871, -0.03419799],\n",
            "       [ 0.09662854, -0.13078168,  0.05016357, -0.13065702,  0.09437933,\n",
            "        -0.08183575,  0.02971463,  0.11602496,  0.02368633,  0.01392001],\n",
            "       [-0.05217277,  0.11208009,  0.01385993, -0.04364265, -0.01423944,\n",
            "        -0.1835063 , -0.05109813, -0.11008358, -0.04454791, -0.01747073],\n",
            "       [-0.17877233,  0.08255945,  0.10435667, -0.19096045, -0.00536476,\n",
            "        -0.04953429, -0.10292279,  0.09040038, -0.06866046, -0.03076947],\n",
            "       [-0.09926146,  0.1326363 ,  0.01790038, -0.11308223,  0.05352156,\n",
            "         0.12321045,  0.1606028 , -0.13545436,  0.00209026,  0.02053734],\n",
            "       [-0.04928133,  0.08930107,  0.16346468,  0.06465065,  0.03019436,\n",
            "        -0.09827609,  0.07063418, -0.00397054, -0.05699549, -0.03093016],\n",
            "       [ 0.01011271,  0.02339575,  0.0159615 ,  0.02604151,  0.13695262,\n",
            "         0.02658517,  0.16890323,  0.04249606,  0.02224006,  0.03510001],\n",
            "       [ 0.12646274,  0.08273702,  0.05017252,  0.11857837, -0.01171662,\n",
            "        -0.1682468 , -0.15763853, -0.14560746,  0.05174275, -0.0296529 ],\n",
            "       [-0.1045348 ,  0.02582216, -0.03236368,  0.09815881, -0.01329115,\n",
            "         0.07500959,  0.01998466,  0.02888813, -0.17840545, -0.12340387],\n",
            "       [-0.0267873 ,  0.10652956, -0.04828882, -0.0418503 ,  0.11392295,\n",
            "         0.06055799, -0.19934702,  0.01227143,  0.13540815, -0.01823097],\n",
            "       [-0.08418786,  0.15167525,  0.02470546,  0.03120163,  0.04375319,\n",
            "         0.05198761,  0.10438373, -0.09834242, -0.00528073,  0.09724155],\n",
            "       [-0.08892366, -0.00752514,  0.15558557,  0.0153236 , -0.09249951,\n",
            "        -0.17469138,  0.11181193, -0.18365365, -0.01006114, -0.03011075],\n",
            "       [-0.05241461,  0.04005743, -0.02128936,  0.08564968, -0.16418476,\n",
            "        -0.02343853, -0.08257004,  0.03556502, -0.03779429,  0.19633628],\n",
            "       [ 0.11591399,  0.01921585, -0.04998795,  0.14049262, -0.06193742,\n",
            "         0.02846304, -0.08954872, -0.10530543,  0.11095388, -0.07447814],\n",
            "       [-0.0419373 ,  0.00261938,  0.09394138, -0.12339294,  0.01863665,\n",
            "         0.11065987,  0.01634923,  0.00400307,  0.03525421,  0.00827915],\n",
            "       [ 0.08982629, -0.12248784, -0.0345291 ,  0.09548294,  0.08020442,\n",
            "        -0.07751378,  0.05850131, -0.05854315, -0.0170362 , -0.14446492],\n",
            "       [ 0.0956689 ,  0.0361592 ,  0.10175939,  0.06460973, -0.05801161,\n",
            "         0.05563949,  0.01342627, -0.00202612,  0.06189683, -0.18079735],\n",
            "       [-0.01123748, -0.13638519,  0.03678123, -0.18954168, -0.01838736,\n",
            "        -0.08813459, -0.08984096,  0.069305  , -0.19137411, -0.17329377],\n",
            "       [ 0.02424528, -0.02457059, -0.02714146, -0.03463542, -0.10806995,\n",
            "         0.00250613, -0.10214501,  0.04062447,  0.04040894, -0.07163757],\n",
            "       [ 0.17474808,  0.03003441, -0.03898354,  0.07150822, -0.03581733,\n",
            "        -0.17237912,  0.067317  ,  0.10257411,  0.14905061, -0.01894584],\n",
            "       [-0.09381071, -0.05810281,  0.04907168, -0.09458999, -0.07028364,\n",
            "         0.00704227, -0.00824617,  0.04696802, -0.03632123, -0.00366011],\n",
            "       [ 0.04890684,  0.09459043, -0.06847533,  0.11914302,  0.13602658,\n",
            "        -0.00359027, -0.00975357, -0.04716687, -0.11464896, -0.09062053],\n",
            "       [-0.03662523, -0.02683449, -0.03050445, -0.06804074, -0.11092681,\n",
            "        -0.13864227, -0.1464584 , -0.16408947,  0.02121332,  0.10416515],\n",
            "       [ 0.13844457, -0.03616305, -0.08601829,  0.05342704, -0.01336921,\n",
            "         0.0905507 ,  0.1097381 ,  0.0121282 , -0.04539812,  0.09169737],\n",
            "       [-0.0228452 , -0.0118421 ,  0.17457367,  0.04881368, -0.118409  ,\n",
            "         0.08724564, -0.0622905 , -0.14499263,  0.18787657, -0.17105812],\n",
            "       [-0.08933427,  0.0714718 , -0.0277504 , -0.06999126,  0.15108815,\n",
            "         0.07632022,  0.03115349, -0.07379549, -0.08359393,  0.03927559],\n",
            "       [-0.04275561, -0.05591248, -0.0347652 ,  0.10193562, -0.06609963,\n",
            "        -0.18852116,  0.02000521, -0.18524098, -0.11603428, -0.0793831 ],\n",
            "       [-0.10091531,  0.08669557,  0.08021174,  0.01214014, -0.03905681,\n",
            "         0.04767161,  0.04781386, -0.02946668,  0.07091182, -0.16232228],\n",
            "       [-0.0228484 ,  0.11016939, -0.09988695,  0.00319305,  0.02856794,\n",
            "         0.13763805,  0.1520892 ,  0.00613005,  0.01347527, -0.03292166],\n",
            "       [-0.0520517 ,  0.00117044, -0.10011237,  0.17746109,  0.06638821,\n",
            "         0.04255855, -0.01288063,  0.03705785,  0.0005398 , -0.09580716],\n",
            "       [-0.02561525,  0.01044632,  0.07851131, -0.14387842,  0.01953858,\n",
            "         0.00959972,  0.08397012,  0.05819265, -0.06900337, -0.02088458],\n",
            "       [-0.07652461, -0.05039322,  0.00950389, -0.02694219,  0.06915688,\n",
            "        -0.04450955, -0.12718467,  0.0156318 ,  0.05268015,  0.00505112],\n",
            "       [ 0.15769427,  0.14032881,  0.10788975, -0.12788251, -0.07428471,\n",
            "         0.05769274, -0.14142054,  0.01244139, -0.05576815, -0.02269803],\n",
            "       [-0.04605881, -0.02355605,  0.03010147, -0.04314532,  0.06967773,\n",
            "         0.03941861, -0.01877524, -0.1146566 , -0.11439262,  0.15840976],\n",
            "       [ 0.10749602, -0.0295113 , -0.11204157,  0.00078147, -0.0695965 ,\n",
            "         0.01522191, -0.0870434 ,  0.12962598,  0.18862292,  0.00505936],\n",
            "       [ 0.08233068, -0.02487208, -0.07487514, -0.06341806,  0.07756525,\n",
            "         0.00620492,  0.03146107, -0.01725645, -0.00088178,  0.12399345],\n",
            "       [ 0.07980763,  0.04470491,  0.08807266, -0.12347644,  0.08217988,\n",
            "        -0.0093841 , -0.01928866,  0.00568103, -0.00102114,  0.07282109],\n",
            "       [ 0.010533  , -0.06610122,  0.00493938,  0.10937091,  0.08118874,\n",
            "         0.07937936, -0.0327356 , -0.18284608, -0.11122715,  0.12031043],\n",
            "       [ 0.03879514,  0.08033306,  0.07163935,  0.03528525, -0.1710811 ,\n",
            "         0.12314942, -0.02940514,  0.17308728, -0.09565739,  0.07935531],\n",
            "       [ 0.08482482, -0.13233577, -0.04391481,  0.03462574,  0.04180957,\n",
            "        -0.00606239,  0.03965548,  0.1007949 , -0.12890443, -0.02863116],\n",
            "       [-0.03232009, -0.14324158, -0.11691276,  0.06475925, -0.04771036,\n",
            "         0.0245206 , -0.01533758,  0.12738235,  0.07735185,  0.09320428],\n",
            "       [-0.11892041,  0.08188327, -0.06279057,  0.01270009,  0.07570415,\n",
            "        -0.12982924,  0.04640988,  0.09456094,  0.05394771, -0.08703184],\n",
            "       [-0.02283698, -0.07471751, -0.02231648, -0.11935478, -0.0481188 ,\n",
            "         0.18898986, -0.06304197, -0.0226851 , -0.04675335,  0.00118985],\n",
            "       [ 0.06814487, -0.03504456, -0.02390348, -0.04177573,  0.04277317,\n",
            "         0.01104366,  0.00215944,  0.09335227, -0.04613429,  0.10567871],\n",
            "       [-0.08256431,  0.0892091 ,  0.03984159, -0.03779468,  0.01872783,\n",
            "         0.05244636, -0.09431761, -0.09092709,  0.01587992, -0.01434686],\n",
            "       [-0.08619146, -0.01469785,  0.03078404, -0.00845716,  0.06481475,\n",
            "        -0.00931884, -0.02787294, -0.1162011 , -0.03251453, -0.10433441]],\n",
            "      dtype=float32)>}\n"
          ]
        }
      ],
      "source": [
        "with tf.name_scope('weights'):\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random.truncated_normal([num_input, num_hidden1], stddev=0.1), name='weight_1'),\n",
        "        'w2': tf.Variable(tf.random.truncated_normal([num_hidden1, num_hidden2], stddev=0.1), name='weight_2'),\n",
        "        'w3': tf.Variable(tf.random.truncated_normal([num_hidden2, num_hidden3], stddev=0.1), name='weight_3'),\n",
        "        'out': tf.Variable(tf.random.truncated_normal([num_hidden3, num_output], stddev=0.1), name='weight_4'),\n",
        "    }\n",
        "\n",
        "print(weights)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E4k_VLzgMN7"
      },
      "source": [
        "The dimension of bias should be a number of neurons in the current layer. For instance, the\n",
        "dimension of bias b2 is the number of neurons in the hidden layer 2. We set the bias value\n",
        "as constant 0.1 in all layers:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "6a8i9LPogMN7"
      },
      "outputs": [],
      "source": [
        "with tf.name_scope('biases'):\n",
        "\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.constant(0.1, shape=[num_hidden1]),name='bias_1'),\n",
        "        'b2': tf.Variable(tf.constant(0.1, shape=[num_hidden2]),name='bias_2'),\n",
        "        'b3': tf.Variable(tf.constant(0.1, shape=[num_hidden3]),name='bias_3'),\n",
        "        'out': tf.Variable(tf.constant(0.1, shape=[num_output]),name='bias_4')\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVpndEC5gMN7"
      },
      "source": [
        "## Forward Propagation\n",
        "\n",
        "Now, we define the forward propagation operation. We use relu activations in all layers\n",
        "and in the last layer we use sigmoid activation as defined below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "9VptXnxmgMN8"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(num_hidden1, activation='relu', input_shape=(num_input,), name='layer1'),\n",
        "    tf.keras.layers.Dense(num_hidden2, activation='relu', name='layer2'),\n",
        "    tf.keras.layers.Dense(num_hidden3, activation='relu', name='layer3'),\n",
        "    tf.keras.layers.Dense(num_output, activation='sigmoid', name='output_layer')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zou_1AbkgMN8"
      },
      "source": [
        "## Compute Loss and Backpropagate\n",
        "\n",
        "\n",
        "\n",
        "Next, we define our loss function. We use softmax cross-entropy as our loss\n",
        "function. Tensorflow\n",
        "provides tf.nn.softmax_cross_entropy_with_logits() function for computing the\n",
        "softmax cross entropy loss. It takes the two parameters as inputs logits and labels.\n",
        "\n",
        "* logits implies the logits predicted by our network. That is, y_hat\n",
        "\n",
        "* labels imply the actual labels. That is, true labels y\n",
        "\n",
        "\n",
        "We take mean of the loss using tf.reduce_mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "IdDtbt2QgMN9"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',  # Using categorical crossentropy for multi-class classification\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3-O61DJgMN-"
      },
      "source": [
        "Now, we need to minimize the loss using backpropagation. Don't worry! We don't have to\n",
        "calculate derivatives of all the weights manually. Instead, we can use tensorflow's\n",
        "optimizer. In this section, we use Adam optimizer. It is a variant of gradient descent\n",
        "optimization technique we learned in the previous chapter. In the next chapter, we will\n",
        "dive into detail and see how exactly all the Adam and several other optimizers work. For\n",
        "now, let's say we use Adam optimizer as our backpropagation algorithm,\n",
        "\n",
        "\n",
        "tf.train.AdamOptimizer() requires the learning rate as input. So we set 1e-4 as the learning rate and we minimize the loss with minimize() function. It computes the gradients and updates the parameters (weights and biases) of our network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "kPaQG6_HgMN-"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7na_ADfgMN-"
      },
      "source": [
        "##  Compute Accuracy\n",
        "\n",
        "We calculate the accuracy of our model as follows.\n",
        "\n",
        "\n",
        "* y_hat denotes the predicted probability for each class by our model. Since we have 10 classes we will have 10 probabilities. If the probability is high at position 7, then it means that our network predicts the input image as digit 7 with high probability.  tf.argmax() returns the index of the largest value. Thus, tf.argmax(y_hat,1) gives the index where the probability is high. Thus, if the probability is high at index 7, then it returns 7\n",
        "<br>\n",
        "\n",
        "\n",
        "* Y denotes the actual labels and it is the one hot encoded values. That is, it consists of zeros everywhere except at the position of the actual image where it consists of 1. For instance, if the input image is 7, then Y has 0 at all indices except at index 7 where it has 1. Thus, tf.argmax(Y,1) returns 7 because that is where we have high value i.e 1.\n",
        "\n",
        "\n",
        "Thus, tf.armax(y_hat,1) gives the predicted digit and tf.argmax(Y,1) gives us the actual digit.\n",
        "\n",
        "tf.equal(x, y) takes x and y as inputs and returns the truth value of (x == y) element-wise. Thus, correct_pred = tf.equal(predicted_digit,actual_digit) consists of True where the actual and predicted digits are same and False where the actual and predicted digits are not the same. We convert the boolean values in correct_pred into float using tensorflow's cast operation. That is, tf.cast(correct_pred, tf.float32). After converting into float values, take the average using tf.treduce_mean().\n",
        "\n",
        "Thus, tf.reduce_mean(tf.cast(correct_pred, tf.float32)) gives us the average correct predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "rhFQD28qgMN_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.9937 - loss: 0.0205 - val_accuracy: 0.9799 - val_loss: 0.0886\n",
            "Epoch 2/7\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9950 - loss: 0.0148 - val_accuracy: 0.9798 - val_loss: 0.0906\n",
            "Epoch 3/7\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9964 - loss: 0.0126 - val_accuracy: 0.9797 - val_loss: 0.1011\n",
            "Epoch 4/7\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9961 - loss: 0.0122 - val_accuracy: 0.9783 - val_loss: 0.1143\n",
            "Epoch 5/7\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9957 - loss: 0.0134 - val_accuracy: 0.9812 - val_loss: 0.0987\n",
            "Epoch 6/7\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9964 - loss: 0.0122 - val_accuracy: 0.9797 - val_loss: 0.1184\n",
            "Epoch 7/7\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9968 - loss: 0.0110 - val_accuracy: 0.9790 - val_loss: 0.1047\n",
            "313/313 - 0s - 1ms/step - accuracy: 0.9790 - loss: 0.1047\n",
            "\n",
            "Test accuracy: 0.9789999723434448\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train_flat, y_train, epochs=7, batch_size=32, validation_data=(x_test_flat, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test_flat, y_test, verbose=2)\n",
        "print(\"\\nTest accuracy:\", test_acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN7mdZPggMN_"
      },
      "source": [
        "## Create Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VAyYLB1gMN_"
      },
      "source": [
        "We can also visualize how the loss and accuracy of our model change during several\n",
        "iterations in tensorboard. So, we use tf.summary() to get the summary of the variable.\n",
        "Since the loss and accuracy are scalar variables, we use tf.summary.scalar() to store the summary as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "yc_jjU5ygMN_",
        "outputId": "7a735108-d6c9-4732-c434-e5263110c331"
      },
      "outputs": [],
      "source": [
        "def log_metrics(epoch, logs):\n",
        "    with summary_writer.as_default():\n",
        "        tf.summary.scalar('Loss', logs['loss'], step=epoch)\n",
        "        tf.summary.scalar('Accuracy', logs['accuracy'], step=epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FT9SZ03KgMN_"
      },
      "source": [
        "Next, we merge all the summaries we use in our graph using tf.summary.merge_all(). We merge all summaries because when we have many summaries running and storing them would become inefficient, so we merge all the summaries and run them once in our session instead of running multiple times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "bf8jy2KvgMOA"
      },
      "outputs": [],
      "source": [
        "log_dir = './logs'\n",
        "summary_writer = tf.summary.create_file_writer(log_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BAn8sOVgMOA"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RAiFKaUgMOA"
      },
      "source": [
        "Now it is time to train our model. As we learned, first we need to initialize all the variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "7S5OxzHegMOA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[ 17.460217   15.074947  -23.056368  ...   6.1282845 -53.25817\n",
            "  -32.698906 ]\n",
            " [ -7.3741813 -44.68805   -36.866104  ... -11.14521   -34.578873\n",
            "   25.547768 ]\n",
            " [  9.91428     1.9410253  -9.128231  ... -17.878704   -2.4646196\n",
            "   38.994423 ]\n",
            " ...\n",
            " [-22.229265   45.167618  -17.548645  ...  13.859132   28.690716\n",
            "    6.6715083]\n",
            " [ 54.546425   -3.9325848   3.0803294 ... -40.231995   16.485685\n",
            "   20.402626 ]\n",
            " [  3.4541411 -55.936886   -6.7378254 ... -18.315048   12.366338\n",
            "   -3.0935564]], shape=(32, 256), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Example explicit variable initialization\n",
        "w = tf.Variable(tf.random.normal(shape=(784, 256)), name='weight')\n",
        "b = tf.Variable(tf.zeros(shape=(256,)), name='bias')\n",
        "\n",
        "# Explicitly assign initial values (if needed)\n",
        "w.assign(tf.random.normal(shape=(784, 256)))\n",
        "b.assign(tf.zeros(shape=(256,)))\n",
        "\n",
        "# Use variables in a model or computation\n",
        "x = tf.random.normal(shape=(32, 784))\n",
        "output = tf.matmul(x, w) + b\n",
        "\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxWaZEm7gMOA"
      },
      "source": [
        "Define the batch size and number of iterations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "oKXWE1iogMOA"
      },
      "outputs": [],
      "source": [
        "batch_size_number = 128\n",
        "num_iterations = 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8yiVVWUgMOB"
      },
      "source": [
        "Start the tensorflow session and perform training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "g_dsZT7agMOB",
        "outputId": "b56d4646-dd7e-476d-e982-ce447f8da076"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 5.6755e-05 - val_accuracy: 0.9839 - val_loss: 0.0991\n",
            "Epoch 2/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 5.4963e-05 - val_accuracy: 0.9839 - val_loss: 0.1015\n",
            "Epoch 3/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.9946e-05 - val_accuracy: 0.9840 - val_loss: 0.1043\n",
            "Epoch 4/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.3221e-05 - val_accuracy: 0.9841 - val_loss: 0.1069\n",
            "Epoch 5/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.9069e-05 - val_accuracy: 0.9841 - val_loss: 0.1092\n",
            "313/313 - 0s - 2ms/step - accuracy: 0.9841 - loss: 0.1092\n",
            "Test accuracy: 0.9840999841690063\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "import datetime  # For unique log directory per run\n",
        "\n",
        "# Set up TensorBoard for logging\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "# Train model using fit method\n",
        "model.fit(x_train.reshape(-1, 784), y_train, epochs=5, batch_size=batch_size_number,\n",
        "          validation_data=(x_test.reshape(-1, 784), y_test),\n",
        "          callbacks=[tensorboard_callback])\n",
        "\n",
        "# Evaluate model\n",
        "test_loss, test_acc = model.evaluate(x_test.reshape(-1, 784), y_test, verbose=2)\n",
        "print(f'Test accuracy: {test_acc}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DYv8RB7gMOS"
      },
      "source": [
        "As you may observe, the loss decreases and the accuracy increases over the training iterations. Now that we have learned how to build the neural network using tensorflow, in the next section we will see how can we visualize the computational graph of our model in tensorboard."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
